# 向 量

数据结构是数据项的结构化集合，其结构性表现为定义于数据项之间的某种逻辑次序。根据这种逻辑次序的复杂程度，大致可以将各种数据结构划分为*线性结构、半线性结构与非线性结构*三大类。
在线性结构中，最为基本的线性结构统称为序列（sequence），根据其中数据项的逻辑次序与其物理存储地址的对应关系不同，又可进一步地将序列区分为向量（vector）和列表（list）。
在向量中，所有数据项的物理存放位置与其逻辑次序完全吻合，此时的逻辑次序也称作秩
（rank）；而在列表中，逻辑上相邻的数据项在物理上未必相邻，而是采用间接定址的方式通过封装后的位置（position）相互引用。

## 从数组到向量

### 数组

一组相关元素各元素之间具有一个线性次序，则可将它们存放于起始于地址A、物理位置连续的一段存储空间，并统称作数组（array），通常以A作为该数组的标识。数组A[]中的每一元素都唯一对应于某一下标编号，通常从0开始编号。

    其中，对于任何0<=i< j < n，A[i]都是A[j]的*前驱*（predecessor），A[j]都是A[i]的*后继*（successor）。
    特别地，对于任何i>=1，A[i-1]称作A[i]的*直接前驱*（immediate predecessor）;对于任何i <= n-2，A[i+1]称作A[i]的*直接后继*（immediate successor）。
    任一元素的所有前驱构成其*前缀*（prefix），所有后继构成其*后缀*（suffix）。

采用这一编号规范使得我们可以直接访问到任意元素，“直接”是指这些对某以元素的增删改查的操作可以在常数时间内完成，只要从首地址A出发，根据元素编号可以经过简单的乘法和加法运算直接计算出元素对应物理地址。
因其中元素的物理地址与其下标之间满足这种线性关系，故亦称作线性数组（linear array）。

### 向量

对数组这一具体数据结构进行抽象和推广，向量（vector）就是线性数组的一种抽象与泛化，由具有线性次序的一组元素构成的集合V = {v0,v1,...,vn-1}，其中的元素分别由秩相互区分。(若元素e的前驱元素共计r个，则其秩就是r)

经如此抽象之后，不再限定同一向量中的各元素都属于同一基本类型，各元素也不见得同时具有某一数值属性，故而并不保证它们之间能够相互比较大小。

## 接口

### ADT接口

作为一种抽象数据类型，向量对象应支持如下操作接口。

操作接口    功能                 适用对象
size() 报告向量当前规模（元素总数） 向量
get(r) 获取秩为r的元素 向量
put(r, e) 用e替换秩为r元素的数值 向量
insert(r, e) e作为秩为r元素插入，原后继元素依次后移 向量
remove(r) 删除秩为r的元素，返回该元素中原存放的对象 向量
disordered() 判断所有元素是否已按非降序排列 向量
sort() 调整各元素的位置，使之按非降序排列 向量
find(e) 查找等于e，且秩最大的元素 向量
search(e) 查找目标元素e，返回不大于e且秩最大的元素 有序向量
deduplicate() 剔除重复元素 向量
uniquify() 剔除重复元素 有序向量
traverse() 遍历向量并统一处理所有元素，处理斱方法由函数对象指定 向量

### Vector模板类

/DSA/Vector.cpp

#### 函数模板

#### 类模板

#### 重载运算符

为什么要重载赋值运算符：对于简单的类，默认的赋值运算符一般就够用了，我们也没有必要再显式地重载它。但是当类持有其它资源时，例如动态分配的内存、打开的文件、指向其他数据的指针、网络连接等，默认的赋值运算符就不能处理了，我们必须显式地重载它，这样才能将原有对象的所有数据都赋值给新对象。
运算符重载函数本质就是将一个运算符与一个类内函数做映射，运算符不再执行他原始的程序，而执行你重载那个函数。重载，不过是一个函数名比较特殊的一个函数，函数名规则是：operator接运算符

由于向量内部含有动态分配的空间，默认的运算符"="不足以支持向量之间的直接赋值。例如，6.3节将以二维向量形式实现图邻接表，其主向量中的每一元素本身都是一维向量，故通过默认赋值运算符，并不能复制向量内部的数据区。为适应此类赋值操作的需求，重载向量的赋值运算符。
代码：/DSA/Vector.cpp/Vector< T >::operator=

#### 构造函数

#### 析构函数

### 空间管理

装填因子：向量实际规模与其内部数组容量的比值（即_size/_capacity），亦称作装填因子（load factor），它是衡量空间利用率的重要指标。
如何才能保证向量的装填因子既不致于超过1，也不致于太接近于0？静态空间显然无法实现，需要改用动态空间管理策略。其中一种有效的方法，即使用所谓的可扩充向量。

#### 可扩充向量

策略：如果数组空间满了，则给她换一个更大的新空间
    为何不直接在后面追加新空间，因为要求物理地址的关系和逻辑地址的关系一致，也就是物理地址也必须相连，显然无法保证在任何时刻已用数组后面都还有足够的空白空间可用来扩容

代码：/DSA/Vector.cpp/Vector< T >::expand()
    注意：由于数组扩容时是新开辟的空间，地址由系统分配，与原数组没有关系，原地址被释放。如果程序是直接操作的数组，就会导致其他地方对数组的引用都变成野指针。现在封装在向量里，向量指针不会变，开辟新空间时新空间地址会直接指向原向量指针，因此避免了野指针的情况发生。

要解决的问题：扩多少
下一节进行分析

#### 分摊分析

可扩容向量的代价是————每次扩容，元素的搬迁都需要花费额外的时间。
对于规模为n的向量，看似每次扩容，都要消耗o(n)的时间，但是由于不可能出现连续扩容的情况，即随着向量规模的不断扩大，在执行插入操作之前需要进行扩容的概率，也将迅速降低。所以不适用于用平均复杂度分析，因此采用分摊分析。
不妨考查对可扩充向量的足够多次连续操作，并将其间所消耗的时间，分摊至所有的操作。如此分摊平均至单次操作的时间成本，称作分摊运行时间（amortized running time）。
以可扩充向量为例，可以考查对该结构的连续n次（查询、插入或删除等）操作，将所有操作中用于内部数组扩容的时间累计起来，然后除以n。只要n足够大，这一平均时间就是用于扩容处理的分摊时间成本。即便排除查询和删除操作而仅考查插入操作，在可扩充向量单次操作中，用于扩容处理的分摊时间成本也不过O(1)。

#### 其他扩容策略

早期可扩充向量多采用另一策略：一旦有必要，则追加固定数目的单元。实际上，无论采用的固定常数多大，在最坏情况下，此类数组单次操作的分摊时间复杂度都将高达o(n)。

#### 缩容

略

### 常规向量

#### 直接引用元素

由于现在封装成了向量结构，导致无法像数组那样便捷的使用下标访问元素，重新恢复这个功能的方法也很简单，那就是重载操作符“[]”，特别的，由于返回的是对数组元素的引用，因此不仅可以取代get()方法，同样也可以取代set()方法。

- 代码

/DSA/Vector.cpp/Vector< T >::operator[]

#### 置乱器

基于恢复直接引用，方便的实现置乱算法：
从待置乱区间的末端开始，向前逐一处理元素。对于当前元素V[i-1],先通过随机函数rand在[0,i)之间随机获取一个数字j，然后通过交换函数swap()将当前元素与V [j]互换。经过O(n)步迭代后实现整体的置乱。

- 代码

/DSA/Vector.cpp/permute()
/DSA/Vector.cpp/Vector< T >::unsort()

#### 判等器与比较器

“判断两个对象是否相等（比对）”与“判断两个对象的相对大小（比较）”都是至关重要的操作，它们直接控制着算法执行的分支方向，因此也是算法的“灵魂”所在。
算法实现的简洁性与通用性，在很大程度上体现于：针对整数等特定数据类型的某种实现，可否推广至可比较或可比对的任何数据类型，而不必关心如何定义以及判定其大小或相等关系。若能如此，我们就可以将比对和比较操作的具体实现剥离出来，直接讨论算法流程本身。
为此，通常可以采用两种方法。其一，将比对操作和比较操作分别封装成通用的判等器和比较器。其二，在定义对应的数据类型时，通过重载"<"和"=="之类的操作符，给出大小和相等关系的具体定义及其判别方法。该书将主要采用后一方式。

在一些复杂的数据结构中，内部元素本身的类型可能就是指向其它对象的指针；对其进行比较所得结果毫无意义。为此，以根据输入是指针还是数值进行重载的方式将这种情况与一般情况予以区分，并且约定在这种情况下，统一按照被指对象的大小做出判断，也就是如果是指针，便取得对应数值传给重新调用的比较函数，使用重载后的数值比较函数进行比较。

- 代码

/DSA/Vector.cpp/Vector< T >::lt/eq()

#### 无序查找

- 无序向量

Vector::find(e)接口，功能语义为“查找与数据对象e相等的元素”。这暗示着，向量元素可通过相互比对判等。比如通过，元素类型T或为基本类型，或已重载操作符“==”或“!=”等方式实现。
这种只能比较元素是否一样，但是元素之间没有大小之分的向量，称作无序向量（unsorted vector）。

- 实现逻辑

在无序向量中查找任意指定元素e时，因为没有更多的信息可以借助，故在最坏情况下，比如向量中并不包含e时，只有在访遍所有元素之后，才能得出查找结论。因此不妨从后往前一个个比对，要么找到相同元素，要么全部比对后查找失败。这种依次逐个比对的查找方式，称作*顺序查找*（sequential search）。
代码：代码：/DSA/Vector.cpp/Vector< T >::find()

- 复杂度

最坏情况下，查找终止于首元素_elem[lo]，运行时间为O(hi - lo) = O(n)。最好情况下，查找命中于末元素_elem[hi - 1]，仅需O(1)时间。
对于规模相同、内部组成不同的输入，渐进运行时间却有本质区别，故此类算法也称作输入敏感的（input sensitive）算法。

#### 插入

插入操作insert(r, e)负责将任意给定的元素e插到任意指定的秩为r的单元。插入之前必须首先调用expand()算法，核对是否即将溢出；若有必要则加倍扩容。
为保证数组元素物理地址连续的特性，随后需要将插入位置之后得元素（如果非空）整体后移一个单元。

注意：这些后继元素**自后向前**的搬迁次序不能颠倒，否则会因元素被覆盖而造成数据丢失。在单元_elem[r]腾出之后，方可执行插入动作。

- 复杂度

主要消耗是后移操作，最多后移n个，复杂度o(n)。

- 代码
/DSA/Vector.cpp/Vector< T >::insert()

#### 删除

删除一般有两种情况，单个删除和连续删除，需要重载为两个接口remove(lo, hi)和remove(r)，表面上看可以通过循环删除单体实现批量删除，仔细思考会发现一个完整的删除操作需要大量的元素位移，因此这么做性能极低。
合适的做法是反过来，实现一个连续删除的功能，单删作为特例引用连删功能实现。如此可将移动操作的总次数控制在O(m)以内，而与待删除区间的宽度无关。

删除操作的具体实现和插入类似，插入是通过后移腾出位置，删除就是通过前移覆盖位置，因此只要找到删除区间内最后一个元素的下一个元素，然后依次自前向后的向前搬运，跨度就是删除区间的大小。
代码：/DSA/Vector.cpp/Vector< T >::remove()

- 复杂度

和插入类似，主要计算成本是移动元素，取决于删除区间得后继元素得数量，和删除元素数量无关。因此平均o(n)。

#### 唯一化（无序）

很多应用环境要求元素具有唯一性，因此需要剔除其中重复的项目。所谓向量的唯一化处理，就是剔除其中的重复元素。

- 实现逻辑

视向量是否有序，该功能有两种实现方式，本节先分析无序向量。
自前向后逐一考查各元素_elem[i]，并通过调用find()接口，在其*前缀*中寻找与之雷同者。若找到，则随即删除；否则，转而考查当前元素的后继。

- 正确性分析

正确性通常通过分析不变性和单调性来证明。

    单调性：问题的有效规模会随着算法的推进不断递减。
    不变性：不仅在算法初始状态下自然满足，而且应与最终的正确性相呼应——当问题的有效规模缩减到0时，不变性应随即等价于正确性。

不变性分析：
在一个查询雷同元素的循环中，当前元素的前缀内所有元素彼此互异。
单调性分析：
待处理元素把数组分为已处理前缀和待处理后缀两部分，每一次查询循环，只会出现两种情况：当前元素于前缀有雷同，根据不变性，至多只有一个，那删除当前元素，后缀前移一位，此时后缀规模-1；当前元素与前缀没有雷同，那将当前元素加入前缀，继续查询下一元素，此时后缀规模同样-1。因此该算法显然总是符合单调性的。
在待后缀规模=0时，根据不变性，正确性显然。

- 复杂度

该算法主要计算成本是循环内部的查前缀和移动后缀操作，加在一起最多是o(n)，根据单调性可以得到循环的时间复杂度是o(n)，因此整体时间复杂度是o(n^2)。

- 代码

/DSA/Vector.cpp/Vector< T >::deduplicate()

#### 遍历

一般情况下有两种情况需要遍历：统一输出，统一修改。实际可能不止这些，因此在遍历时一般需要对元素进行一个基本操作，为了灵活性，实现可以将操作类型作为参数传入遍历函数的效果，可以选择使用函数对象（操作器）或者函数指针。

    函数对象：首先是一个对象，但是这类对象的操作符“()”经重载之后，在形式上等效于一个函数接口，[对象名（参数）]，因此使用形式和函数一致。它封装了一个或多个操作，同时可以作为一个对象被当作参数传递给其他函数。在遍历功能中，操作器被用作遍历向量的函数的参数，它定义了如何对向量中的每个元素进行操作。
    相比较而言，函数对象形式的功能更强，适用范围更广。比如，函数对象的形式支持对向量元素的关联修改，因为函数对象可以封装状态信息，这意味着它可以在遍历向量的过程中记录一些信息，并在后续的操作中使用这些信息。例如可以定义一个函数对象，它在遍历向量时记录最大值和最小值，然后根据这些信息来修改其他元素的值。也就是说，对各元素的修改不仅可以相互独立地进行，也可以根据某个（些）元素的数值相应地修改另一元素。
    函数对象和函数指针的核心区别是：函数对象作为一个类或结构体的实例，存在于原函数的整个周期，而函数指针调用完返回后就释放了。

- 实现逻辑

对遍历函数传入操作器或者函数指针，在遍历过程中将每个元素作为参数传给操作器或者函数指针。在函数指针的方案中，该指针函数只有一个参数，其类型为对向量元素的引用，故通过该函数即可直接访问或修改向量元素。

- 复杂度

遍历本身只有一层循环，也就是o(n)，循环内部对元素基本操作为常数级时间复杂度，因此总时间复杂度为o(n)。

- 实例

在向量类模板的方法中有一个自增1的功能，因此可以通过遍历函数实现全体自增1。即在自增函数内部调用遍历函数，遍历函数调用+1的基本操作。
复杂度：一次循环遍历，循环内部只有常数级基本操作，所以总复杂度为o(n)。

- 代码
/DSA/Vector.cpp/Vector< T >::traverse ( VST& visit )    //函数对象
/DSA/Vector.cpp/Vector< T >::traverse ( void ( *visit ) ( T& ) ) //函数指针

#### 特别篇（参数/引用/变量）

在C++中，*和&有两个含义，当在声明变量时，他们是变量类型标记的一部分，注意是一部分，当使用参数时，他们是运算符。

##### 声明变量时

“int \* p”这句的含义是声明一个叫做p的变量，变量类型是“int类型的指针”，在C++中空格没影响，所以星号写在哪边都可以，但是如果根据往常一样写成int \*p；容易错误的理解成：声明一个指针p，变量类型是int，事实上没有一个具体的变量类型叫做“指针”，只有“int类型指针”，“float类型指针”这种变量类型，所以科学的写法应该是“int\* p”。&同理。

而在作为变量类型中，*和&的区别是，指针类型是一个独立的一般的变量类型，可以独立存在，可以没有赋值，可以是空值，可以重新修改赋值，但是引用类型不可以，引用类型只是一个“变量名别名”，因此首先得存在一个变量名，才可能还存在一个别名，因此引用名和原变量是共生的，因此也不可以修改引用对象。引用类型变量的使用也和原普通变量完全一致。

##### 单独使用时

*是解引用运算符，可以理解为间接寻址，也就是说对应变量里存的是地址。
&是取地址运算，是获取对应变量所在的物理地址。

### 有序向量

有时候有的数据列表中元素天然有序或者时刻保持有序，比如班级学号表、比赛排名等。对于这种序列的操作，显然会有一些不同。

#### 比较器

有序向量不仅需要判等，还需要能够比较大小，不然便没有所谓的顺序。对于一些复杂的无法直接比较的元素，最常见的就是在内部指定某一（些）可比较的数据项，并由此确立比较的规则。这里假设复杂数据对象已经重载了"<"和"<="等操作符。

#### 有序性甄别

作为无序向量的特例，有序向量可以直接使用无需向量的查找算法，但这显然是对向量有序性的特性的的浪费。有序向量的查找、唯一化等操作都可更快地完成。因此需要先判断向量的有序性，以选取更高效的算法。

- 实现逻辑

在冒泡排序中使用是否存在逆序的思想判断排序的完成情况，同样可以使用在这里。具体方法就是设置一个无序对的计数器i默认为0，然后逐一对比相邻的元素，如果n个元素，就要对比n-1次。当前者大于后者时计数器+1。如果i=0则表示升序有序，如果i=n-1则表示降序有序，否则表示无序。

- 代码

/DSA/Vector.cpp/Vector< T >::disordered()

#### 唯一化（有序）

相对于无序向量，有序向量中清除重复元素的操作更为重要。，在清除无序向量中的重复元素时，事实上如果先有序化再清除效率会更高。

##### 低效版

- 实现逻辑

利用有序元素的特性：有序向量的重复元素必然是前后紧邻的，因此只需要检查相邻元素是否相同。如果相同，调用remove()删除后者然后进行下一个比较，如果不同直接进行下一个比较。如此，扫描结束后向量中将不再含有重复元素。

- 效率和问题

这里的运行时间，主要消耗于while循环，共需迭代_size - 1 = n - 1步，o(n)复杂度。而循环内部的remove接口的调用，取决于重复元素数量，在假设的最坏情况下，n个元素都相同，循环中每次都要删除，都要后面元素前移，（n-2）+(n-3)+……+2+1=o(n^2)。这一效率竟与向量未排序时相同，说明该方法未能充分利用此时向量的有序性。

- 代码

/DSA/Vector.cpp/Vector< T >::uniquify()

##### 高效版

- 改进思路

稍加分析可以发现过高的复杂度源于对remove接口的调用，而对于一个有序向量，如果存在多个相同元素，也必然是相连的，因此只需要直接区间移除后续全部相同元素而不必一次一次清除。

- 实现逻辑

设置两个变量i和j，分别指向第一个和第二元素。i和j进行比对，相同时j++，继续比对，直到不同时，把第一个不同的，也就是当前的j复制到i+1的位置，然后i++，这就完成了一个元素的去重。
然后当前的i是一个新元素，当前的j处在新元素的第一个的位置，重复i和j的比较，直到j走完最后一个。

- 代码

/DSA/Vector.cpp/Vector< T >::uniquify1B()

- 复杂度

while循环的每一步迭代，仅需对元素数值做一次比较，向后移动一到两个位置指针，并至多向前复制一个元素，故只需常数时间。而在整个算法过程中，每经过一步迭代秩j都必然加一，鉴于j不能超过向量的规模n，故共需迭代n次。由此可知，uniquify()算法的时间复杂度应为O(n)，较之uniquifySlow()的O(n^2)，效率整整提高了一个线性因子。
反过来，在遍历所有元素之前不可能确定是否有重复元素，故就渐进复杂度而言，能在O(n)时间内完成向量的唯一化已属最优。当然，之所以能够做到这一点，关键在于向量已经排序。

#### 查找

有序向量的有序性会让查找变得很有意思。
为区别于无序向量的查找接口find()，有序向量的查找接口将统一命名为search()。
接口代码：/DSA/Vector.cpp/Vector< T >::search()
以下研究两类典型的查找算法。

##### 二分查找A

- 实现逻辑（减而治之）

循秩访问的特点加上有序性，使得我们可将“减而治之” 策略运用于有序向量的查找。以任一元素S[mid] = x为界，都可将区间分为三部分，且根据此时的有序性必有：S[lo, mid) <= S[mid] <= S(mid, hi)，hi取size，也就是元素数量，由于秩从0开始，元素数量对应的位置是最后一个元素的下一个位置，既减少了运算，又实现了半开区间，直接mid=lo+hi取中值也可以在n为奇数时正好取到中间无须在做额外处理。
然后只需要将目标元素和x进行比较，就会知道目标元素处于哪个区间内，同时意味着可以直接排除另一个区间。当然若x=目标元素则直接命中结果，查找结束。并且通过引入lo、hi和mi等变量，将减治算法通常的递归模式改成了迭代模式。

    也就是说，每经过至多两次比较操作，我们或者已经找到目标元素，或者可以将查找问题简化为一个规模更小的新问题。如此，借助递归机制即可便捷地描述和实现此类算法。实际上，以下将要介绍的各种查找算法都可归入这一模式，不同点仅在于其对切分点mi的选取策略，以及每次深入递归之前所做比较操作的次数。

- 复杂度

以上算法采取的策略可概括为，以“当前区间内居中的元素”作为目标元素的试探对象。即使是从最坏的角度看，这一策略也每次都能排除掉一半元素。也就是说随着算法的运行，待解决问题的规模是以当前规模1/2的速度减少。经过至多log2(hi - lo)步迭代后，算法必然终止。总时间复杂度为O(logn)。

    log2n复杂度：每一步导致问题规模缩减一半，也就是一步使问题规模等于n/2，两步使问题规模成为(N/2)/2=N/2^2,k步使问题规模变成n/(2^k)。问题解决时问题规模要变成1。也就是问题规模的解决速度和2^k是一个数量级，n~2^k。而时间复杂度就是考察问题规模和执行时间的关系，而执行时间就和需要执行的代码数量直接相关，这里是k，由n~2^k得，k~log2n,于是时间复杂度k~T(n)=o(log2n)。

- 代码

/DSA/Vector.cpp/Vector< T >::binSearchA()

###### 查找长度

以上算法涉及到的计算包括元素比较和秩的操作，秩的操作必然是常数级，元素可能很复杂，比较操作可能远大于常数级，而查找算法的整体效率也更主要地取决于其中所执行的**元素大小比较操作的次数，即所谓查找长度**(search length)。
通常，可针对查找成功或失败等情况，从最好、最坏和平均情况等角度，分别测算查找长度，并凭此对查找算法的总体性能做一评估。

- 成功查找长度

对于长度为n的有序向量，共有n种可能的成功查找，分别对应于某一元素。每一种成功的结果对应的查找长度只与目标的秩和问题规模n有关，于元素本身具体的值无关。
由于在该算法中先比较小于后比较大于，如果都失败最后才是等于，所以如果是成功的查找，都要先失败两次。

    假设一组有序元素S[ ],n=7，目标元素的r=0，它必然经历查找路径：S[3]（左）-> S[1]（左）->s[0]（左）->s[0]（右）四次比较,所以成功查找长度=4。
    以此类推各元素所对应的成功查找长度分别应为：{ 4, 3, 5, 2, 5, 4, 6 }
    假定查找的目标元素按等概率分布，则平均查找长度即为：(4 + 3 + 5 + 2 + 5 + 4 + 6) / 7 = 4.14。

为了估计出一般情况下的成功查找长度，仍在等概率条件下考查长度为n = 2^k-1的有序向量，并将其对应的平均成功查找长度记作Caverage(k)，将所有元素对应的查找长度总和记作C(k) = Caverage(k)∙(2^k- 1)。

    特别地，当k = 1时向量长度n = 1，成功查找仅有一种长度为2的情况，故有边界条件：caverage(1) = C(1) = 2。

    之所以选择n = 2^k-1，是为了方便计算：当长度为此时，每次二分都会将向量分成两个等份的子列。为什么2^K-1可以:首先2^K-1肯定是一个奇数，去掉中值后左右两边肯定相等，然后要做的就是保证分区之后左右那个相等的数，也是2^m-1的结构，因此只需要假设一个子列是2^m-1，那么他的原向量就是2(2^m-1)+1=2^(m+1)-2+1=2^(m+1)-1。设m+1=k，则原向量大小为2^k-1。

以下采用递推分析法。对于长度为n = 2^k- 1的有序向量，每步迭代都有三种可能的分支：经过1次成功的比较后，转化为一个规模为2^(k-1) - 1的新问题（左侧分支）；经2次失败的比较后，终止于向量中的某一元素，并确认在此处成功命中；经1次失败的比较另加1次成功的比较后，转化为另一个规模为2^(k-1) - 1的新问题（右侧分支）。
因此C(k)长度=左分支节点成功总长度+进入左分支需要长度+右分支总长度+进入右分支需要长度+中间值直接成功的长度。
C(1)=2
C(2)=C(1)+1+C(1)+2+2=9
C(3)=C(2)+4+C(2)+5+2=29
现在的难点是每个分支消耗的比较次数，可以注意到在全部的成功查找中必然每次分支都会找到一个元素，因此有2^k-1-1个分支（去掉第一个中间元素），在这些分支中有一半要比较1次，有一半要比较两次。
C(k)= [C(k - 1) + (2^(k-1) - 1)] + 2 + [C(k - 1) + 2*(2^(k-1) - 1)]（式2-1）
= 2∙C(k - 1) + 3∙2^(k-1)- 1

    从下往上递推，查找到每个目标元素时，需要消耗2次比较，剩下的都是进入该分支消耗的比较，因此总长度就是元素数量*2+所有分支消耗的次数。
    C(1)=1*2+0
    C(2)=(2^2-1)*2+1+2=9

………………浪费半天时间才搞清楚一点点，略过

平均查找长度应为：O(1.5k) = O(1.5∙log2n)

- 失败查找长度

失败节点就是每个存在的元素的缝隙的位置，也就是n个元素有n+1个失败节点，。因此，失败查找的时间复杂度应为确定的o(logn)。一般情况下的平均失败查找长度亦为O(1.5∙log2n)。

- 不足

尽管二分查找算法（版本A）即便在最坏情况下也可保证O(logn)的渐进时间复杂度，但就其常系数1.5而言仍有改进余地。可以发现不同情况产生的查找分支的长度最大有两倍差距，十分不均匀。可以从这个角度考虑优化。

#### Fibonacci查找

- 递推方程

既然是分支长度差异大产生的的不平衡导致的整体效率低。为改进以上二分查找算法的版本A，不妨从刻画查找长度随向量长度递推变化的式2-1入手。

更准确地讲，主要取决于(2^(k-1)- 1)和2 * (2^(k-1)- 1)两项，其中的(2^(k-1)- 1)为子向量的宽度，而系数1和2则是左分支、右分支的子向量，所需做的比较操作次数。之所以存在均衡性方面的缺陷，根源正在于这两项的大小不相匹配。

基于这一理解，不难找到解决问题的思路，具体地不外乎两种：
其一，调整前、后区域的宽度，适当地加长（缩短）前（后）子向量
其二，统一沿两个方向深入所需要执行的比较次数，比如都统一为一次。后一思路的实现将在稍后介绍，以下首先介绍前一思路的具体实现。

- 黄金分割

实际上，减治策略本身并不要求子向量切分点mi必须居中，故按上述改进思路，不妨按黄金分割比来确定mi。为简化起见，不妨设向量长度n = fib(k) - 1。

于是fibSearch(e, 0, n)查找可以mi = fib(k - 1) - 1作为前、后子向量的切分点。如此，前、后子向量的长度将分别是：
fib(k - 1) - 1
fib(k - 2) - 1 = (fib(k) - 1) - (fib(k - 1) - 1) - 1
于是，无论朝哪个方向深入，新向量的长度从形式上都依然是某个Fibonacci数减一，故这一处理手法可以反复套用，直至因在S[mi]处命中或向量长度收缩至零而终止。这种查找算法，亦称作Fibonacci查找（Fibonaccian search）。

- 代码

/DSA/Vector.cpp/Vector< T >::fibSearch()

仔细地学习和分析暂时跳过

#### 二分查找（版本B）

- 从三分支到两分支

上面分析已经知道两种优化方向，通过Fibonacci选中点实现偏移中点以平衡左右分支的转入开销，也可以直接采用另一种方式，让左右分支的转入比较次数的消耗都一样，都是1。
相应的，分支结果也不再是有三种，而只有两种（不会直接命中中点）。

- 实现逻辑

首先，每一步迭代只需判断是否e < A[mi]，即可相应地更新有效查找区间的右边界（hi = mi）或左边界（lo = mi）。另外，只有等到区间的宽度已不足2个单元时迭代才会终止，最后再通过一次比对判断查找是否成功。

- 代码

/DSA/Vector.cpp/Vector< T >::binSearchB()

- 性能分析

在这一版本中，只有在向量有效区间宽度缩短至1个单元时算法才会终止，而不能如版本A那样，一旦命中就能及时返回。
因此，最好情况下的效率有所倒退。当然，作为补偿，最坏情况下的效率相应地有所提高。
实际上无论是成功查找或失败查找，版本B各分支的查找长度更加接近，故整体性能更趋稳定。

- 不足

有时元素内部相同元素有权重高低，而现在当有多个目标时不能选择返回真正的目标元素，另外查找失败也只能简单的返回一个-1标识，若能返回失败的地点，方便后续插入。

#### 二分查找（版本C）

- 实现逻辑

在版本B的基础上进行改进，首先是边界条件，从本来的命中区间大小1变成0，这样会让最后一个元素也参与比较。

另外，转入右端分支的左区间取做mi+1而不是mi。因为并不需要再比较mi，表面上看存在风险。此时只能确定切分点A[mi] <= e，“贸然”地将A[mi]排除在进一步的查找范围之外，似乎可能因遗漏这些元素，而导致本应成功的查找以失败告终。

版本C中的循环体，具有如下不变性：
A[0, lo)中的元素皆不大于e；A[hi, n)中的元素皆大于e。

由于只判断小于，然后无论结果是什么，都要切割掉mi，进入左侧或右侧区间，由于左侧必然小于e，右侧必然大于e，那么最后的lo停留的位置，要么在目标右侧，此时lo-1是目标，要么在大于e的最小元素，此时lo-1是小于e的最大元素。e只需要插在lo位置。
同样的，在循环结束时，无论成功与否，都只需要返回lo-1。

- 代码

/DSA/Vector.cpp/Vector< T >::binSearch()

- 思考

如何实现的同时可以返回成功或失败的位置？没想明白是怎么想出这个算法的。
